{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88714, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_path = './data/word_emb.json'\n",
    "with open(emb_path) as f:\n",
    "    embedding_matrix = json.load(f)\n",
    "embedding_matrix = np.asarray(embedding_matrix, dtype=np.float32)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 #\"Learning rate\"\n",
    "batch_size = 100 #\"Batch size to use\"\n",
    "dropout = 0 #1-keep_prob\n",
    "context_len = 600 #\"The maximum context length of your model\"\n",
    "question_len = 30 #\"The maximum question length of your model\"\n",
    "(VOCAB_SIZE, embedding_size) = embedding_matrix.shape\n",
    "hidden_size = 200 #\"Size of the hidden states\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = layers.Input(shape=(context_len, ))\n",
    "question = layers.Input(shape=(question_len, ))\n",
    "\n",
    "context_embs = layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                            output_dim=embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)(context)\n",
    "qn_embs = layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                            output_dim=embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)(question)\n",
    "\n",
    "masking_layer = layers.Masking()\n",
    "masked_context_embs = masking_layer(context_embs)\n",
    "masked_qn_embs = masking_layer(qn_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_layer = layers.GRU(hidden_size, dropout=dropout, return_sequences=True) \n",
    "backward_layer = layers.GRU(hidden_size, dropout=dropout, return_sequences=True, go_backwards=True)\n",
    "bidirect_gru = layers.Bidirectional(forward_layer, \n",
    "                                    backward_layer=backward_layer)\n",
    "context_hiddens = bidirect_gru(masked_context_embs)\n",
    "question_hiddens = bidirect_gru(masked_qn_embs)\n",
    "\n",
    "attn_output = layers.Attention()([context_hiddens, question_hiddens])\n",
    "attn_output = layers.Dropout(dropout)(attn_output)\n",
    "blended_reps = layers.concatenate([context_hiddens, attn_output], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_reps_final = layers.Dense(units=hidden_size,\n",
    "                                  activation='relu')(blended_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Shape_1:0' shape=(3,) dtype=int32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(context_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_start = layers.Dense(units=1, activation=None)(blended_reps_final)\n",
    "logits_start = tf.squeeze(logits_start, axis=[2])\n",
    "prob_dist_start = logits_start #layers.Softmax(axis=1)(logits_start)\n",
    "\n",
    "logits_end = layers.Dense(units=1, activation=None)(blended_reps_final)\n",
    "logits_end = tf.squeeze(logits_end, axis=[2]) \n",
    "prob_dist_end = logits_end #layers.Softmax(axis=1)(logits_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    #return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs=[context, question], outputs=[prob_dist_start, prob_dist_end])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 600)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 600, 300)     26614200    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 30, 300)      26614200    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               multiple             0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) multiple             602400      masking[0][0]                    \n",
      "                                                                 masking[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 600, 400)     0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 600, 400)     0           attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 600, 800)     0           bidirectional_2[0][0]            \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 600, 200)     160200      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 600, 1)       201         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 600, 1)       201         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_4 (TensorFl [(None, 600)]        0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_5 (TensorFl [(None, 600)]        0           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 53,991,402\n",
      "Trainable params: 763,002\n",
      "Non-trainable params: 53,228,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans_span = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "\n",
    "model.fit(x=None, \n",
    "          y=ans_span, \n",
    "          batch_size=batch_size, \n",
    "          epochs=1, \n",
    "          verbose=1, \n",
    "          callbacks=None) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
